# TinyLlama-Hindi
We finetune the TinyLlama 1.1B model to improve its proficiency in Hindi using a low-cost and low-resource pipeline. This involves expanding the modelâ€™s tokenizer to 57,104 tokens by adding 25,104 Hindi-specific tokens, followed by continued pretraining on a large Hindi corpus and supervised fine-tuning using instruction datasets in Hindi. 
