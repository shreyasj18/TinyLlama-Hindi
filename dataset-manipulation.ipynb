{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 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","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"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\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:04:55.195495Z","iopub.execute_input":"2025-03-21T17:04:55.195945Z","iopub.status.idle":"2025-03-21T17:04:56.536010Z","shell.execute_reply.started":"2025-03-21T17:04:55.195902Z","shell.execute_reply":"2025-03-21T17:04:56.534781Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\n\ndef extract_data(examples, dataset_type):\n    questions, answers = [], []\n\n    if dataset_type in [\"anudesh\", \"hh-rlhf\"]:\n        for message_list in examples[\"messages\"]:\n            q_batch, a_batch = [], []\n            if isinstance(message_list, list):\n                for convo in message_list:\n                    if isinstance(convo, dict):\n                        if convo.get(\"role\") == \"user\":\n                            q_batch.append(convo.get(\"content\", \"\"))\n                        elif convo.get(\"role\") == \"assistant\":\n                            a_batch.append(convo.get(\"content\", \"\"))\n            questions.append(q_batch)\n            answers.append(a_batch)\n\n    elif dataset_type == \"dolly\":\n        questions = [[q] for q in examples[\"instruction\"]]\n        answers = [[a] for a in examples[\"response\"]]\n\n    elif dataset_type == \"flan_v2\":\n        questions = [[q] for q in examples[\"inputs\"]]\n        answers = [[a] for a in examples[\"targets\"]]\n\n    elif dataset_type == \"nmt-seed\":\n        questions = [[q] for q in examples[\"input_text\"]]\n        answers = [[a] for a in examples[\"output_text\"]]\n\n    # Filter out empty entries\n    filtered_questions = [q for q, a in zip(questions, answers) if q and a]\n    filtered_answers = [a for q, a in zip(questions, answers) if q and a]\n\n    return {\"question\": filtered_questions, \"answer\": filtered_answers}\n\n# Load and combine all subsets\ndatasets_to_merge = [\n    (\"anudesh\", \"hi\"),\n    (\"dolly\", \"hi\"),\n    (\"flan_v2\", \"hi\"),\n    (\"hh-rlhf\", \"hi\"),\n    (\"nmt-seed\", \"hi\")\n]\n\ndf_list = []\n\nfor subset, split in datasets_to_merge:\n    ds = load_dataset(\"ai4bharat/indic-instruct-data-v0.1\", subset, split=split)\n\n    print(f\"Processing {subset}... Total Rows: {len(ds)}\")\n    \n    extracted_data = ds.map(lambda x: extract_data(x, subset), batched=True)\n\n    # Check for valid data before appending\n    questions = sum(extracted_data[\"question\"], [])\n    answers = sum(extracted_data[\"answer\"], [])\n\n    # Align lengths\n    min_length = min(len(questions), len(answers))\n    questions, answers = questions[:min_length], answers[:min_length]\n\n    if questions and answers:\n        df = pd.DataFrame({\n            \"question\": questions,\n            \"answer\": answers\n        })\n        df_list.append(df)\n    else:\n        print(f\"⚠️ Warning: No valid data found for {subset}. Skipping...\")\n\n# Combine all data into a single DataFrame\nfinal_df = pd.concat(df_list, ignore_index=True)\n\n# Convert DataFrame to Hugging Face Dataset\nfinal_dataset = Dataset.from_pandas(final_df)\n\n# Push the dataset to Hugging Face Hub\nfinal_dataset.push_to_hub(\"shreyas18/Hindi_instruct_v2.1\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:18:12.119368Z","iopub.execute_input":"2025-03-14T18:18:12.119986Z","iopub.status.idle":"2025-03-14T18:20:30.113974Z","shell.execute_reply.started":"2025-03-14T18:18:12.119944Z","shell.execute_reply":"2025-03-14T18:20:30.112947Z"}},"outputs":[{"name":"stdout","text":"Processing anudesh... Total Rows: 7577\nProcessing dolly... Total Rows: 15011\nProcessing flan_v2... Total Rows: 67463\nProcessing hh-rlhf... Total Rows: 5000\nProcessing nmt-seed... Total Rows: 50000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc493d74f7064cba93eb9d2ed1e960d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/152 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aba7f80bade745ae8ecb286dab1616a5"}},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/datasets/shreyas18/Hindi_instruct_v2.1/commit/ad2ca933dd72c423589fb038e75c07e64013b567', commit_message='Upload dataset', commit_description='', oid='ad2ca933dd72c423589fb038e75c07e64013b567', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/shreyas18/Hindi_instruct_v2.1', endpoint='https://huggingface.co', repo_type='dataset', repo_id='shreyas18/Hindi_instruct_v2.1'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\n\n# Data Extraction Function for lm-sys\ndef extract_data_lm_sys(examples):\n    questions, answers = [], []\n    \n    for msgs in examples[\"messages\"]:\n        q_list, a_list = [], []\n        if isinstance(msgs, list):\n            for convo in msgs:\n                if isinstance(convo, dict):\n                    role = convo.get(\"role\")\n                    content = convo.get(\"content\", \"\").strip()\n\n                    if content:\n                        if role == \"user\":\n                            q_list.append(content)\n                        elif role == \"assistant\":\n                            a_list.append(content)\n        \n        # Append entries even if only one side exists to prevent data loss\n        questions.append(q_list if q_list else [\"\"])\n        answers.append(a_list if a_list else [\"\"])\n\n    return {\"question\": questions, \"answer\": answers}\n\n# Load lm-sys dataset\nlm_sys_ds = load_dataset(\"ai4bharat/indic-instruct-data-v0.1\", \"lm_sys\", split=\"hi\")\n\n# Extract Data\nextracted_data = lm_sys_ds.map(lambda x: extract_data_lm_sys(x), batched=True, batch_size=100)\n\n# Concatenate conversation turns into single entries\nquestions = [\" \".join(q) for q in extracted_data[\"question\"]]\nanswers = [\" \".join(a) for a in extracted_data[\"answer\"]]\n\n# Ensure both lists have the same length\nmin_len = min(len(questions), len(answers))\nquestions, answers = questions[:min_len], answers[:min_len]\n\n# Create DataFrame\nlm_sys_df = pd.DataFrame({\"question\": questions, \"answer\": answers})\n\n\n# Append lm-sys data to existing dataset\nexisting_ds = load_dataset(\"shreyas18/Hindi_instruct_v2.1\", split=\"train\")\ncombined_df = pd.concat([existing_ds.to_pandas(), lm_sys_df], ignore_index=True)\n\n# Push the updated dataset to Hub\nmerged_ds = Dataset.from_pandas(combined_df)\nmerged_ds.push_to_hub(\"shreyas18/Hindi_instruct_v2.1\", split=\"train\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:27:07.627550Z","iopub.execute_input":"2025-03-14T18:27:07.627946Z","iopub.status.idle":"2025-03-14T18:27:39.397587Z","shell.execute_reply.started":"2025-03-14T18:27:07.627916Z","shell.execute_reply":"2025-03-14T18:27:39.396464Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/321 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06a3774057944a9b75b81bee2037f05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/159M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f70ec59010c4914be68735b3d18e80b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/171866 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"661ca10780a84460b37f41449cdc781d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9aecaa3f1856409d85e2150014bd81f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/111 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d33bbbe04fa443c89de35f53a102a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/111 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cca76e148804c3f90f679def5c7d0f8"}},"metadata":{}},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/datasets/shreyas18/Hindi_instruct_v2.1/commit/c6e42d0a1c3e10bd4257acd4258d77e9d6dd0122', commit_message='Upload dataset', commit_description='', oid='c6e42d0a1c3e10bd4257acd4258d77e9d6dd0122', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/shreyas18/Hindi_instruct_v2.1', endpoint='https://huggingface.co', repo_type='dataset', repo_id='shreyas18/Hindi_instruct_v2.1'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"len(merged_ds )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-14T18:28:29.392403Z","iopub.execute_input":"2025-03-14T18:28:29.392856Z","iopub.status.idle":"2025-03-14T18:28:29.399226Z","shell.execute_reply.started":"2025-03-14T18:28:29.392824Z","shell.execute_reply":"2025-03-14T18:28:29.398328Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"221866"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\nfrom huggingface_hub import HfApi\n\n# Load dataset in streaming mode to save memory\ndataset_name = \"shreyas18/Hindi_instruct_750k\"\ndataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n\n# Convert dataset to a DataFrame in an efficient way\ndef stream_to_df(dataset, chunk_size=10000):\n    data = []\n    for i, example in enumerate(dataset):\n        data.append({\"question\": example[\"question\"], \"answer\": example[\"answer\"]})\n        if (i + 1) % chunk_size == 0:\n            yield pd.DataFrame(data)\n            data = []\n    if data:\n        yield pd.DataFrame(data)\n\n# Efficiently write the streamed dataset into multiple smaller DataFrames\ndf_list = []\nfor df_chunk in stream_to_df(dataset):\n    df_list.append(df_chunk)\ndf = pd.concat(df_list, ignore_index=True)  # Final merge\n\n# Define multiple new questions and answers\nnew_data_pairs = [\n    (\"आप कौन हैं?\", \"नमस्ते! मैं हिंदीलामा हूँ, आपका अपना हिंदी चैटबॉट। मैं आपकी किस प्रकार सहायता कर सकता हूँ?\"),\n    (\"आपकी उम्र क्या है?\", \"मेरा कोई निश्चित उम्र नहीं है, लेकिन मैं हमेशा मदद के लिए तैयार हूँ!\"),\n    (\"आप क्या कर सकते हैं?\", \"मैं सवालों के जवाब दे सकता हूँ, जानकारी प्रदान कर सकता हूँ, और संवाद कर सकता हूँ!\"),\n    (\"तुम कौन हो?\", \"नमस्ते! मैं हिंदीलामा हूँ, आपका अपना हिंदी चैटबॉट। मैं आपकी किस प्रकार सहायता कर सकता हूँ?\"),\n    (\"आपका क्या नाम है?\", \"नमस्ते! मैं हिंदीलामा हूँ, आपका अपना हिंदी चैटबॉट। मैं आपकी किस प्रकार सहायता कर सकता हूँ?\"),\n    (\"तुम्हारा नाम क्या है\", \"नमस्ते! मैं हिंदीलामा हूँ, आपका अपना हिंदी चैटबॉट। मैं आपकी किस प्रकार सहायता कर सकता हूँ?\"),\n    (\"तुम क्या हो?\", \"नमस्ते! मैं हिंदीलामा हूँ, आपका अपना हिंदी चैटबॉट। मैं आपकी किस प्रकार सहायता कर सकता हूँ?\")\n]\n\n# Use a generator to avoid large memory allocation\ndef generate_new_data(pairs, num_repeats=200):\n    for q, a in pairs:\n        for _ in range(num_repeats):\n            yield {\"question\": q, \"answer\": a}\n\n# Convert new data directly into a DataFrame\nnew_data_df = pd.DataFrame(generate_new_data(new_data_pairs, num_repeats=200))\n\n# Append new data efficiently\ndf = pd.concat([df, new_data_df], ignore_index=True)\n\n# Convert back to Hugging Face dataset format efficiently\nupdated_dataset = Dataset.from_pandas(df)\n\n# Push the dataset in smaller chunks to avoid memory overload\napi = HfApi()\nupdated_dataset.push_to_hub(\"shreyas18/Hindi_instruct_750k_1\", split=\"train\", max_shard_size=\"500MB\")\n\nprint(\"✅ Optimized dataset processing completed and pushed to Hugging Face successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:16:25.528531Z","iopub.execute_input":"2025-03-21T19:16:25.528975Z","execution_failed":"2025-03-21T19:19:19.344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Dataset, DatasetDict\nfrom huggingface_hub import HfApi\n\n# Load existing dataset from Hugging Face\ndataset_name = \"shreyas18/Hindi_instruct_v2.1\"\ndataset = load_dataset(dataset_name)\ndf_1 = pd.DataFrame(dataset['train'])\n\n# Display the last 10 rows\nprint(df_1.tail(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T17:27:08.152212Z","iopub.execute_input":"2025-03-21T17:27:08.152594Z","iopub.status.idle":"2025-03-21T17:27:26.253228Z","shell.execute_reply.started":"2025-03-21T17:27:08.152558Z","shell.execute_reply":"2025-03-21T17:27:26.251982Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/321 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aebd2c43e7dc4b5eba88247444271060"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9799bf2de53497f90e64af149006574"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/112M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"736c2ead627b478b91cc90e58b205176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/222266 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb3e1d9b88b54654a3a988a14e11b6cb"}},"metadata":{}},{"name":"stdout","text":"                               question  \\\n222256  आपको सबसे ज़्यादा क्या पसंद है?   \n222257  आपको सबसे ज़्यादा क्या पसंद है?   \n222258  आपको सबसे ज़्यादा क्या पसंद है?   \n222259  आपको सबसे ज़्यादा क्या पसंद है?   \n222260  आपको सबसे ज़्यादा क्या पसंद है?   \n222261  आपको सबसे ज़्यादा क्या पसंद है?   \n222262  आपको सबसे ज़्यादा क्या पसंद है?   \n222263  आपको सबसे ज़्यादा क्या पसंद है?   \n222264  आपको सबसे ज़्यादा क्या पसंद है?   \n222265  आपको सबसे ज़्यादा क्या पसंद है?   \n\n                                                   answer  \n222256  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222257  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222258  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222259  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222260  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222261  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222262  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222263  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222264  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n222265  मैं एक AI मॉडल हूँ। मेरे पास भावनाएँ या व्यक्त...  \n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import load_dataset, Dataset\n\n# Load the existing dataset\nexisting_dataset = load_dataset(\"shreyas18/Hindi_instruct_v2.1\")\nexisting_data = pd.DataFrame(existing_dataset['train'])\n\n# List of abusive words\nabusive_words = [\n    \"आंड़\", \"आंड\", \"आँड\", \"बहनचोद\", \"बेहेनचोद\", \"भेनचोद\", \"बकचोद\", \"बकचोदी\", \"बेवड़ा\", \"बेवड़े\",\n    \"बेवकूफ\", \"भड़ुआ\", \"भड़वा\", \"भोसड़ा\", \"भोसड़ीके\", \"भोसड़ीकी\", \"भोसड़ीवाला\", \"भोसड़ीवाले\", \"भोसरचोदल\",\n    \"भोसदचोद\", \"भोसड़ाचोदल\", \"भोसड़ाचोद\", \"बब्बे\", \"बूबे\", \"बुर\", \"चरसी\", \"चूचे\", \"चूची\", \"चुची\",\n    \"चोद\", \"चुदने\", \"चुदवा\", \"चुदवाने\", \"चूत\", \"चूतिया\", \"चुटिया\", \"चूतिये\", \"चुत्तड़\", \"चूत्तड़\",\n    \"दलाल\", \"दलले\", \"फट्टू\", \"गधा\", \"गधे\", \"गधालंड\", \"गांड\", \"गांडू\", \"गंडफट\", \"गंडिया\", \"गंडिये\",\n    \"गू\", \"गोटे\", \"हग\", \"हग्गू\", \"हगने\", \"हरामी\", \"हरामजादा\", \"हरामज़ादा\", \"हरामजादे\", \"हरामज़ादे\",\n    \"हरामखोर\", \"झाट\", \"झाटू\", \"कुत्ता\", \"कुत्ते\", \"कुतिया\", \"कुत्ती\", \"लेंडी\", \"लोड़े\", \"लौड़े\",\n    \"लौड़ा\", \"लोड़ा\", \"लौडा\", \"लिंग\", \"लोडा\", \"लोडे\", \"लंड\", \"लौंडा\", \"लौंडे\", \"लौंडी\", \"लौंडिया\",\n    \"लुल्ली\", \"मार\", \"मारो\", \"मारूंगा\", \"मादरचोद\", \"मादरचूत\", \"मादरचुत\", \"मम्मे\", \"मूत\", \"मुत\",\n    \"मूतने\", \"मुतने\", \"मूठ\", \"मुठ\", \"नुननी\", \"नुननु\", \"पाजी\", \"पेसाब\", \"पेशाब\", \"पिल्ला\", \"पिल्ले\",\n    \"पिसाब\", \"पोरकिस्तान\", \"रांड\", \"रंडी\", \"सुअर\", \"सूअर\", \"टट्टे\", \"टट्टी\", \"उल्लू\"\n]\n\n# Generate new abusive rows\nnew_data = []\nfor word in abusive_words:\n    for _ in range(100):\n        new_data.append({\n            \"question\": word,\n            \"answer\": \"कृपया सम्मानजनक भाषा का प्रयोग करें। हम बेहतर संवाद के लिए यहां हैं।\"\n        })\n\n# Convert new data to DataFrame\nnew_df = pd.DataFrame(new_data)\n\n# Append new data to the existing dataset\nupdated_data = pd.concat([existing_data, new_df], ignore_index=True)\n\n# Convert back to Hugging Face dataset format and push\nupdated_dataset = Dataset.from_pandas(updated_data)\nupdated_dataset.push_to_hub(\"shreyas18/Hindi_instruct_v2.1\", split=\"train\")\n\nprint(\"Dataset updated and appended successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:41:16.964887Z","iopub.execute_input":"2025-03-21T18:41:16.965536Z","iopub.status.idle":"2025-03-21T18:42:14.144088Z","shell.execute_reply.started":"2025-03-21T18:41:16.965483Z","shell.execute_reply":"2025-03-21T18:42:14.143108Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc3d3b27ac1c4ad88d5f038e0d437dc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/112M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ff35fa64fb4fd68df4517ab60f84e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/223256 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df73f409c24443429b89099d06bfffae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefe80497483495a99c8e0e05ea23c14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/118 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a747b59cd1364ba9baec274c1ba4cabd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/118 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db6a48bf2c94466a82d8182be14c3378"}},"metadata":{}},{"name":"stdout","text":"Dataset updated and appended successfully!\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, concatenate_datasets\n\n# Load both datasets\ndataset1 = load_dataset(\"atharvanighot/Hindi-Instruct-500K\", split=\"train\")\ndataset2 = load_dataset(\"shreyas18/Hindi_instruct_v2.1\", split=\"train\")\n\n# Merge datasets\nmerged_dataset = concatenate_datasets([dataset1, dataset2])\n\n# Push merged dataset to Hugging Face Hub\nmerged_dataset.push_to_hub(\"shreyas18/Hindi_instruct_750k\")\n\nprint(\"Datasets merged and pushed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:48:28.105350Z","iopub.execute_input":"2025-03-21T18:48:28.105775Z","iopub.status.idle":"2025-03-21T18:50:13.667477Z","shell.execute_reply.started":"2025-03-21T18:48:28.105749Z","shell.execute_reply":"2025-03-21T18:50:13.666283Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/545 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"458fe646afd64878ae0014b64fddc747"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00006.parquet:   0%|          | 0.00/156M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aad8cdc13a024aaf987fac3e65cf5b9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00006.parquet:   0%|          | 0.00/155M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca4979bf38c84c50bbc98d6d430f6e7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00006.parquet:   0%|          | 0.00/153M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4248a270b88947f1b246f45d05fd12c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00006.parquet:   0%|          | 0.00/206M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c34abf3b7de642d38fae2b7cfec1c482"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00006.parquet:   0%|          | 0.00/207M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86e21fca015f46929cc267d9e0e76a5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00006.parquet:   0%|          | 0.00/206M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a55794b4bd469da0baba664af2cdd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/508609 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"603fc9f2ba03471fafcd2c31592f9e7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/321 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61a527cf3fa24b13948ae6b6cd2008aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00002.parquet:   0%|          | 0.00/132M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763d983d52bb4744a93aeb1e4e176da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00002.parquet:   0%|          | 0.00/111M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5407911be2b1484ca2c5fbdd197c9b95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/234356 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d18dc732864d96a1f212a8db63e2c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e50d6f44a594269a36d9458509127fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ecfc4f11674448295582fe1a8c715a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ad7684dd3947ebb1d0e99e976f60ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eeb84758d314531a5297f195b3a0eb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a621708d863c4b1284ce26b6cd9b8edc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee4d871bf664719841028ff1d86e58c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a828453f9d4147d09d35339745ac5c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f903c3fa794c4f9bb8e9cb84963583"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a33dfddcb02440fb719b8509895e347"}},"metadata":{}},{"name":"stdout","text":"Datasets merged and pushed successfully!\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\n\n# Load the dataset\ndataset = load_dataset(\"shreyas18/Hindi_instruct_750k\", split=\"train\")\n\n# Remove the unwanted column\ndataset = dataset.remove_columns([\"__index_level_0__\"])\n\n# Push the cleaned dataset back to Hugging Face\ndataset.push_to_hub(\"shreyas18/Hindi_instruct_750k\")\n\nprint(\"Dataset cleaned and updated successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T18:55:37.988851Z","iopub.execute_input":"2025-03-21T18:55:37.989565Z","iopub.status.idle":"2025-03-21T18:57:38.919623Z","shell.execute_reply.started":"2025-03-21T18:55:37.989532Z","shell.execute_reply":"2025-03-21T18:57:38.918569Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/369 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f16051e7ba584165a17a14283dd11fd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00008.parquet:   0%|          | 0.00/170M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73eab54e1bd2461d92daa5c9e8fae5af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00001-of-00008.parquet:   0%|          | 0.00/170M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb35d7abd71493b84ab6a6c7ed380e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00002-of-00008.parquet:   0%|          | 0.00/182M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95d8d3bb5e6d42a18860c86651d83c1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00003-of-00008.parquet:   0%|          | 0.00/226M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5c7bef8ae5e4dedaaccb9f9a783e307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00004-of-00008.parquet:   0%|          | 0.00/225M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdb2faa9a0634ba7853c1b0416723e37"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00005-of-00008.parquet:   0%|          | 0.00/172M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"883d209627a748c3841998251a645a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00006-of-00008.parquet:   0%|          | 0.00/72.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94b034842a944ac9b0d749115cf3e8af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00007-of-00008.parquet:   0%|          | 0.00/107M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d73b820c9f574ab283654f91cd71cbe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/742965 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd7e14d4bdff4e298ac59068b02a963c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Uploading the dataset shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a2d73b010641e4b627161acedd312c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ee90572e1f249089a91ae5d2a78cfb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ded3fcb53bf4ef68fde46e13076c8e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447f41ac1b6f441abea3bc5307b9ba43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52331df53656470fb53532b1ad2449c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1ab284aaafc4ebeb304c289461a58ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6102cdef8f214cefb41fd2d82196d36b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d7c31ee22774b2b909d2865936cc447"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Creating parquet from Arrow format:   0%|          | 0/93 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a6f96d95094d94806a70790fbfd040"}},"metadata":{}},{"name":"stdout","text":"Dataset cleaned and updated successfully!\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}